defmodule AiAgentAdvisor.Agent do
  alias AiAgentAdvisor.Accounts.User
  alias AiAgentAdvisor.Ingestion.Document
  alias AiAgentAdvisor.Repo
  alias Ecto.Vector.Vector

  import Ecto.Query

  @embedding_model "gemini-embedding-001"
  @chat_model "gemini-flash-latest"

  @doc """
  The main entry point for asking the agent a question.
  """
  def ask(%User{} = user, question) when is_binary(question) do
    # 1. Embed the user's question to get a query vector.
    with {:ok, query_vector} <- embed_text(question),
         # 2. Find relevant documents in the database using the vector.
         context_documents <- find_relevant_documents(user, query_vector),
         # 3. Build the prompt and ask the LLM for the final answer.
         {:ok, answer} <- generate_completion(question, context_documents) do
      # For now, we return the answer directly. Later, this will send to the ChatLive process.
      answer
    else
      {:error, reason} ->
        IO.inspect(reason, label: "Error in Agent.ask")
        "Sorry, I encountered an error. Please try again."
    end
  end

  defp embed_text(text) do
    case Gemini.embedding(model: @embedding_model, text: text) do
      {:ok, %{embedding: %{value: values}}} ->
        {:ok, Vector.new(values)}

      {:error, reason} ->
        {:error, reason}
    end
  end

  defp find_relevant_documents(%User{} = user, %Vector{} = query_vector) do
    query =
      from(d in Document,
        where: d.user_id == ^user.id,
        order_by: [asc: l2_distance(d.embedding, ^query_vector)],
        limit: 5,
        select: d.content
      )

    Repo.all(query)
  end

  defp generate_completion(question, context_documents) do
    context_str = Enum.join(context_documents, "\n\n")

    system_prompt = """
    You are a helpful assistant for a financial advisor.
    Based ONLY on the context provided below, answer the user's question.
    Do not use any outside knowledge. If the answer is not in the context, say you don't know.
    """

    prompt = """
    CONTEXT:
    #{context_str}

    QUESTION:
    #{question}
    """

    messages = [
      %{role: "user", parts: [%{text: prompt}]}
    ]

    case Gemini.generate_content(
           model: @chat_model,
           contents: messages,
           system_instruction: %{parts: [%{text: system_prompt}]}
         ) do
      {:ok, %{candidates: [%{content: %{parts: [%{text: text}]}}]}} ->
        {:ok, text}

      {:error, reason} ->
        {:error, reason}
    end
  end
end
